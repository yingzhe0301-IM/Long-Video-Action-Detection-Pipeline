# Long Video Action Detection Pipeline

---

## ğŸš€ Project Goal

This project delivers a lightweight, reproducible **Detector-Analyzer** intelligent system for **longâ€‘video action detection**â€”identifying events in hoursâ€‘long fishing footage.  
The demo walks through the full flow:

1. Batched, asynchronous frame loading  
2. YOLOâ€‘based inference on MPS or CUDA  
3. Aggregating detections into an **actionâ€‘representative signal**  
4. Executing custom postâ€‘detection algorithms (peak or interval detection)  
5. Plotting & exporting results for quick inspection 

![Pipeline workflow](diagram/workflow.png)

![Pipeline demo](diagram/haul_demo.gif)


---

## ğŸ”§ Prerequisites

| Item            | Requirement     |
|-----------------|-----------------|
| **OS**          | macOS or Ubuntu |
| **Environment** | Miniconda       |

---

## ğŸ“š Required Scripts

- `fixed_experiments.py` â€” main entry point for singleâ€‘action demo  
- `config.py` â€” global paths & environment settings  
- `experiment_config.py` â€” actionâ€‘specific hyperâ€‘parameters  
- `common_experiments.py` â€” shared experiment helpers  
- `batch_utils.py` â€” batchâ€‘construction utilities  
- `detection_utils.py` â€” postâ€‘processing & metric computation  
- `video_batch_processor_unified.py` â€” unified batch inference engine  
- `video_processor_async.py` â€” asynchronous frame loader  
- `action_detection.py` â€” lowâ€‘level detection wrapper  

**Included folders**

```
model_weights/           # Pretrained weights (e.g., pumping.pt)
selected_test_video/     # Demo clips grouped by action type
plot/                    # Output directory for detection plots
```

---

## ğŸ› ï¸ Setup

Run the oneâ€‘click script at the repository root:

```bash
bash install_dependencies.sh
```

The script will:

1. **Create** (or reuse) a Conda environment named **`haul_env`** with PythonÂ 3.12  
2. **Detect** your OS (macOSÂ MPS vs. UbuntuÂ CPU/GPU) and install the correct PyTorch build  
3. **Install** all remaining packages from **`requirements.txt`**

If you prefer a manual install, open the script and execute the commands stepâ€‘byâ€‘step.

---

## âš¡ï¸ Run a Pumping Detection Demo

```bash
python fixed_experiments.py \
  --single \
  --action_type pumping \
  --frame_skip 4 \
  --video_root selected_test_video \
  --model_weight model_weights/pumping.pt
```

This command will:

1. **Load** frames asynchronously in batches  
2. **Infer** detections with YOLO  
3. **Compute** an actionâ€‘representative signal (detectionsÂ /Â frame)  
4. **Apply** postâ€‘detection algorithms (peak or interval detection)  
5. **Save** plots & CSV to `plot/latest/`  
6. **Done!** Review results in the output folder  

After the run, open **`plot/latest/`** to view detection timelines, signals, and summary CSV.

---

## ğŸ§ª R&D Process & Key Experiments

1. **Visual primitives & raw detections**  
   Accurately recognising a high-level action in a long video first requires detecting the *visual primitives* that uniquely characterise that action â€” for example, a **fish-net**, deck personnel, or the fish themselves.  
   *Illustration* â†“

   ![Sparse fish-net detection](diagram/sparse_fishnet.jpg)

2. **Formulating an action-representative signal**  
   Our signal design was guided by two principles:  
   1. **Computational thrift** â€” it must be derivable directly from raw detections with negligible overhead.  
   2. **Human interpretability** â€” when model accuracy is acceptable, the signal shape should allow a human to infer the action visually.  
   We compared two candidate formulations (see figure) and ultimately adopted **average detections per frame**, offering a robust compromise between noise suppression and temporal fidelity.
   
   ![Signal candidates](diagram/action_representative_signal.png)

3. **Temporal reasoning via post-detection analysis**  
   To recover the *duration* of an action such as fishing, we detect local maxima in the signal that correspond to the **net cast** and **net retrieval** events.  
   Each odd-even peak pair brackets a single action interval; peak prominence and minimum-distance thresholds are carefully tuned (see below).  
   ![Peak detection](diagram/signal_detection.png)

4. **Frame-skip versus accuracy trade-off**  
   A ten-minute clip at 30 fps contains 18 000 frames; inferring on every frame is cost-prohibitive.  
   Experiments show that a **frame-skip of 4â€“5** â€” sampling roughly 20 â€“ 25 % of frames â€” still achieves **100 % recall** across 48 validation videos.  
   ![Accuracy vs cost](diagram/fixed_accuracy_vs_inference_cost_recommended_skip.png)

5. **Dynamic frame-skip strategy (DNFS)**  
   We prototyped a dynamic scheme that predicts imminent object absence and skips accordingly, with a short rewind on positive detections.  
   Despite its conceptual elegance, DNFS introduces additional latency and GPU load because both predictor and detector must co-reside on the device â€” making it unsuitable for edge deployment where resources are limited.

   ![Runtime comparison](diagram/comparison_runtime_vs_cost.png)

6. **Deployment readiness**  
   With the detector, temporal logic, and frame-sampling policy validated, we quantised the model to **INT8** and benchmarked it on representative edge hardware, laying the groundwork for production deployment.

## ğŸ’¡ Technical Highlights

- **Highâ€‘throughput frame loader** â€” `video_processor_async.py` uses *asyncio* + prefetch queues to maximise GPU utilisation.  
- **Unified batch inference** â€” `video_batch_processor_unified.py` autoâ€‘adjusts batch size for your GPU.  
- **Action signal & postâ€‘detection** â€” perâ€‘frame counts feed peak/interval detectors to pinpoint events.  
- **Falseâ€‘positive reduction** â€” actionâ€‘specific thresholds and signal smoothing eliminate spurious detections.  
- **Modular configuration** â€” tweak everything in `config.py` & `experiment_config.py`.  

---
